---
layout: default
---

<body>	
	
<div class="content heading anchor" id="home">
		<table style="margin-top: -4mm;">
		</table>
        <div style="margin-top: 0mm;" class="img">
			<img class="header-img" src="images/Anyi.jpg" alt="Photo" align="left" height="250" >
		</div>
        <div style="margin-top: 7mm;" class="header-text">
            <h2>Anyi Rao</h2>
            <p>
				Postdoctoral Scholar <br>
				Computer Science Department <br>
				Stanford University <br>
				Ph.D. MMLab CUHK<br>
				Email: anyirao [at] stanford.edu <br>
            </p>
		<div id="contact">	
			<a href="https://scholar.google.com/citations?user=8lKr7j4AAAAJ&hl=en&authuser=1&oi=ao" class="icon">
          			<img src="images/google_scholar_30.jpg"  alt="View Anyi Rao's profile on Google Scholar">
          		</a>
              		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          		<a href="https://github.com/AnyiRao" class="icon">
          			<img src="images/git_30.jpg"  alt="View Anyi Rao's codes on Github">
          		</a>
			&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			<a href="https://www.linkedin.com/in/anyirao/" class="icon">
                		<img src="images/linkedin_30.jpg" alt="View Anyi Rao's profile on LinkedIn">
            		</a>
        </div>
	  </div>
</div>	   

<table hegiht="50"><td hegiht="50"></td></table>
<table hegiht="50"><td hegiht="50"></td></table>
<table hegiht="50"><td hegiht="50"></td></table>

	   
<div class="content anchor" id="bio">     
	<h3 class="content-section">Bio</h3>
		<p> Anyi Rao is a Postdoctoral Scholar at 
			<a href="https://www.stanford.edu/">Stanford</a>
			with 
			<a href="http://graphics.stanford.edu/~maneesh/">Maneesh Agrawala</a>.
			He has research experiences at <a href="https://about.meta.com/realitylabs/">Meta Reality Lab</a>, 
			<a style="color:#333" href="https://vectorinstitute.ai">Vector Institute</a>, 
			<a style="color:#333" href="https://web.cs.toronto.edu">University of Toronto</a>, 
			<a style="color:#333" href="https://www.cs.hku.hk/">Hong Kong University</a>.
			He received the Ph.D. at <a href="http://mmlab.ie.cuhk.edu.hk/">MMLab</a> 
			in the 
			<a style="color:#333" href="http://csrankings.org/#/index?vision&world">Chinese University of Hong Kong</a>,
			advised by <a href="http://dahua.me/">Dahua Lin</a> and <a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou</a>. 
			He got the B.S. from 
			<a style="color:#333" href="http://csrankings.org/#/index?ai&world">Nanjing University</a>
			in 2018, ranking 1/183 in EE.
			He studies human-centered AI for multimodality and creativity,
			with focuses on intelligent video editing and creation, video semantic and cinematic analysis, 
			aiming to build connections between AI and humans for collaborative intelligence.
			<br>
			
			<!-- 
			He was fortunate to work with 
			<a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>, 
			<a style="color:#333" href="https://web.cs.toronto.edu/">UofT</a> & 
			<a style="color:#333" href="https://vectorinstitute.ai/">Vector Institute</a>,
			<a href="http://ix.cs.uoregon.edu/~dou/">Dejing Dou</a>,
			<a href="http://ix.cs.uoregon.edu/~lowd/">Daniel Lowd</a>
			at <a style="color:#333" href="http://aimlab.cs.uoregon.edu/publication.php">AIM Lab</a>, 
			and <a href="http://www.cs.hku.hk/people/profile.jsp?teacher=fcmlau">Francis Lau</a>
			at <a style="color:#333" href="https://www.cs.hku.hk/">HKU CS</a>. -->
		</p>
</div>   
	
<div class="content anchor" id="news">  
	<h3 class="content-section-list">News</h3>
	<p>
	<li>
		2023-01: One  <a href="https://arxiv.org/abs/2302.09018">paper</a> is accepted to AAAI 2023 as Oral.
		<br>
	<li>
		2022-10:
		We are organizing <a href="https://cveu.github.io/">The Second Workshop on AI for Creative Video Editing and Understanding</a> at ECCV 2022.
		Researchers, artists, video directors, and entrepreneurs from academia (Stanford, Berkeley), industry (Adobe, Netflix, Meta) and more, 
		are going to share and sparkle their ideas together!
		Please follow our <a href="https://twitter.com/cveu_workshop">Twitter</a> for more information!
		<br>
	<li>
		2022-07: Two papers on <a href="https://city-super.github.io/">City Research</a>: 
		<a href="https://city-super.github.io/citynerf/">CityNeRF</a> and 
		 <a href="https://city-super.github.io/shoot360/">Shoot360</a> are accepted to ECCV 2022 and SIGGRAPH 2022.
		<br>
	<li>
		2022-03: Two papers are accepted to CVPR 2022 and IEEE Transactions on Multimedia.
		<br>
	<li>
		2021-09: We are organizing <a href="https://cveu.github.io/">The First Workshop on AI for Creative Video Editing and Understanding</a> during ICCV 2021. 
		<br>
	<li>2021-07: Two papers are accepted to ICCV 2021 and IEEE Transactions on Multimedia.
		<br>
	<li> 2021-05: Our CVPR 2020 work <a href="https://github.com/AnyiRao/SceneSeg">SceneSeg</a>
		is set as the baseline for the 
		<a href="https://algo.qq.com/"> 2021 Tencent Advertising Algorithm Competition and ACM Multimedia 2021 Grand Challenge</a>
		Track 1 Video Ads Content Structuring.
		Come to join and win USD$100,000 for the first prize.
		<br>
	<li> 2020-07: <a href="https://movienet.github.io/">MovieNet</a> 
		is online with an easy-to-use <a href="https://github.com/movienet/movienet-tools">toolkit</a> 
		as a part of <a href="http://openmmlab.com/"> OpenMMLab</a>.
		<br>
	<li> 2020-07: Three papers are accepted to ECCV 2020.
		<br>
	<li> 2020-02: One paper is accepted to CVPR 2020.
		Also appears at <a href="https://sites.google.com/view/luv2020/home"> LUV 2020</a> (15-min talk)
		and <a href="http://sightsound.org"> Sight and Sound 2020</a> (5-min talk).
		<br>
	<li> 2020-01: <a href="https://www.aclweb.org/anthology/P18-2006"> HotFlip </a> is included in <a href="https://allennlp.org"> AllenNLP</a>
		<iframe src="https://img.shields.io/github/stars/allenai/allennlp?style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
		and 
		<a href="https://github.com/QData/TextAttack"> TextAttack</a>
		<iframe src="https://img.shields.io/github/stars/QData/TextAttack?style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
		<br>
	</li>
	</p>
</div>   
	
<div class="content anchor" id="publication">     
	<h3  class="content-section">
	<a style="color:#333333" href="./publication">Selected Publication</a>
	</h3>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
			  <td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/arxiv23vds.png" width="160"  border="0"></p>
			</td>
			<td  valign="middle">
			<p><span style="font-weight: bold;"> Dynamic Storyboard Generation in an Engine-based Virtual Environments for Video Production </span><br>
			 <span style="font-weight: bold;">Anyi Rao*</span>, Xuekun Jiang*, Yuwei Guo, Linning Xu, Lei Yang, Libiao Jin, Dahua Lin, Bo Dai
				<br>
				Arxiv, 2023<br>
				<span class="tag"><a href="https://arxiv.org/abs/2301.12688">[Paper]</a></span>
				<span class="tag"><a href="https://virtualfilmstudio.github.io/">[Webpage]</a></span>
				</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
			  <td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/aaai23pstl.png" width="160"  border="0"></p>
			</td>
			<td  valign="middle">
			<p><span style="font-weight: bold;"> Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences</span><br>
			  Yujie Zhou, Haodong Duan, <span style="font-weight: bold;">Anyi Rao</span>, Bing Su, Jiaqi Wang
				<br>
				AAAI Conference on Artificial Intelligence
				(<span style="font-weight: bold;">AAAI</span>), 2023 <font color="#e86e14">(Oral)</font> <br>
				<span class="tag"><a href="https://arxiv.org/abs/2302.09018">[Paper]</a></span>
				<span class="tag"><a href="https://github.com/YujieOuO/PSTL.git">[Webpage]</a></span>
				</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
			  <td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/siggraph22shoot360.png" width="160"  border="0"></p>
			</td>
			<td  valign="middle">
			<p><span style="font-weight: bold;"> Shoot360: Normal View Video Creation from City Panorama Footage	</span><br>
			 <span style="font-weight: bold;">Anyi Rao</span>, Linning Xu, Dahua Lin
				<br>
				ACM Special Interest Group on Computer Graphics and Interactive Techniques Conference
				(<span style="font-weight: bold;">SIGGRAPH</span>), 2022 <br>
				<span class="tag"><a href="https://dl.acm.org/doi/abs/10.1145/3528233.3530702">[Paper]</a></span>
				<span class="tag"><a href="https://city-super.github.io/shoot360/">[Webpage]</a></span>
				</td>
	</tbody>
	</table>

	<table style="margin-top: -2mm;"  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
				 <td width="160">
			<p align="center" style="margin-top:3mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/eccv22bungeenerf.png" width="160"  border="0"></p>
			</td>
			<td  valign="middle">
			<p><span style="font-weight: bold;">BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering</span> 
			<br>
				Also known as <span style="font-weight: itaic;">CityNeRF: Building NeRF at City Scale</span> 
			<br>
			Yuanbo Xiangli*, Linning Xu*, Xingang Pan, Nanxuan Zhao, <span style="font-weight: bold;">Anyi Rao</span>, Christian Theobalt, Bo Dai, Dahua Lin
			<br>
			European Conference on Computer Vision
				(<span style="font-weight: bold;">ECCV</span>), 2022 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2112.05504"> [Paper]</a></span>
			<span class="tag"><a href="https://city-super.github.io/citynerf">[Webpage]</a></span>
			</td>
		</tbody>
	</table>

	
	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
			<td width="160">
				<p align="center" style="margin-top:2mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class="">
				<img src="./images/papers/tmm22unscreen.png" width="160"  border="0"> </p>
			</td>
			<td  valign="middle">
				<p><span style="font-weight: bold; margin-top:5mm">  A Coarse-to-Fine Framework for Automatic Video Unscreen </span><br>
				<span style="font-weight: bold;">Anyi Rao</span>, Linning Xu, Zhizhong Li, Qingqiu Huang, Zhanghui Kuang, Wayne Zhang, Dahua Lin 
				<br>
				IEEE Transactions on Multimedia, 
				(<span style="font-weight: bold;">TMM</span>), 2022 <br>
				<span class="tag"><a href="https://ieeexplore.ieee.org/document/9709668">[Paper]</a></span>
				<span class="tag"><a href="https://anyirao.com/files/projects/tmm22unscreen/">[Webpage]</a></span>
				</p>
			</td>
	</tbody>
	</table>


	<table style="margin-top: 0mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
			  <td width="160">
			<p align="center" style="margin-top:2mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/iccv21city.png" width="160"  border="0"></p>
			</td>
			<td  valign="middle">
			<p><span style="font-weight: bold;">BlockPlanner: City Block Generation with Vectorized Graph Representation </span><br>
				Linning Xu*, Yuanbo Xiangli*, <span style="font-weight: bold;">Anyi Rao</span>, Nanxuan Zhao, Bo Dai, Ziwei Liu, Dahua Lin
				<br>
			IEEE/CVF International Conference on Computer Vision
			(<span style="font-weight: bold;">ICCV</span>), 2021 <br>
			<span class="tag"><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_BlockPlanner_City_Block_Generation_With_Vectorized_Graph_Representation_ICCV_2021_paper.pdf">[Paper]</a></span>
			<span class="tag"><a href="https://city-super.github.io/blockplanner/">[Webpage]</a></span>
			</td>
	</tbody>
	</table>

	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
			  <td width="160">
			<p align="center" style="margin-top:4mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class="">
				<img src="./images/papers/tmm21shot.jpg" width="160"  border="0"> </p>
			</td>
			<td  valign="middle">
			<p><span style="font-weight: bold;">Jointly Learning the Attributes and Composition of Shots for Boundary Detection in Videos </span><br>
				Xuekun Jiang, Libiao Jin, <span style="font-weight: bold;">Anyi Rao</span><sup>+</sup>(corresponding), Linning Xu, Dahua Lin 
				<br>
				IEEE Transactions on Multimedia, 
				(<span style="font-weight: bold;">TMM</span>), 2021 <br>
				<span class="tag"><a href="https://ieeexplore.ieee.org/abstract/document/9464668/">[Paper]</a></span>
				<span class="tag"><a href="https://zweipa.github.io/TMM_SCTSNet/">[Webpage]</a></span>
				</td>
	</tbody>
	</table>

	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
			  <td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/eccv20shot.png" width="160"  border="0"></p>
			</td>
			<td  valign="middle">
			<p><span style="font-weight: bold;">A Unified Framework for Shot Type Classification Based on Subject Centric Lens
				</span><br>
		<span style="font-weight: bold;">Anyi Rao</span>, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, Dahua Lin
				<br>
			European Conference on Computer Vision
			(<span style="font-weight: bold;">ECCV</span>), 2020 <br>
			Also appears at <a href="https://dramaqa.snu.ac.kr/Workshop/2020"> Video Turing Test 2020</a> (5-min talk) <br>
			<span class="tag"><a href="https://arxiv.org/abs/2008.03548">[Paper]</a></span>
			<span class="tag"><a href="./projects/ShotType.html">[Webpage]</a></span>
			</td>
	</tbody>
	</table>

	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
			  <td width="160">
			<p align="center" style="margin-top:3mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/eccv20movienet.png" width="160"  border="0"></p>
			</td>
			<td  valign="middle">
			<p><span style="font-weight: bold;">MovieNet: A Holistic Dataset for Movie Understanding 
			</span><br>
				Qingqiu Huang, Yu Xiong, <span style="font-weight: bold;">Anyi Rao</span>, Jiaze Wang, Dahua Lin
				<br>
				European Conference on Computer Vision
				(<span style="font-weight: bold;">ECCV</span>), 2020 <font color="#e86e14">(Spotlight)</font> <br>
				<span class="tag"><a href="https://arxiv.org/abs/2007.10937">[Paper]</a></span>
				<span class="tag"><a href="http://movienet.github.io/">[Webpage]</a></span>
				<!-- <span class="tag"><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490681-supp.pdf">[Supplements]</a></span>
				<span class="tag"><a href="https://github.com/movienet/movienet-tools">[Code]</a></span> -->
				</td>
	</tbody>
	</table>

	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
			  <td width="160">
			<p align="center" style="margin-top:2mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/cvpr20sceneseg.png" width="160"  border="0"></p>
			</td>
			<td  valign="middle">
			<p><span style="font-weight: bold;">A Local-to-Global Approach to Multi-modal Movie Scene Segmentation </span><br>
			<span style="font-weight: bold;">Anyi Rao</span>, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, Dahua Lin
				<br>
			IEEE/CVF Conference on Computer Vision and Pattern Recognition 
			(<span style="font-weight: bold;">CVPR</span>), 2020 <br>
			Also appears at <a href="https://sites.google.com/view/luv2020/home"> LUV 2020</a> (15-min talk)
			and <a href="http://sightsound.org/"> Sight and Sound 2020</a> (5-min talk) <br>
			<span class="tag"><a href="https://arxiv.org/abs/2004.02678">[Paper]</a></span>
			<span class="tag"><a href="./projects/SceneSeg.html">[Webpage]</a></span>
				 </td>
		</tbody>
	</table>
	
	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
			<tbody><tr valign="baseline">
				  <td width="160">
				<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/eccv20person.png" width="160"  border="0"></p>
				</td>
				<td  valign="middle">
				<p><span style="font-weight: bold;">Online Multi-modal Person Search in Videos	</span><br>
			 	Jiayue Xia, <span style="font-weight: bold;">Anyi Rao</span><sup>+</sup>(corresponding), Linning Xu, Qingqiu Huang, Dahua Lin
					<br>
					European Conference on Computer Vision
					(<span style="font-weight: bold;">ECCV</span>), 2020 <br>
					<span class="tag"><a href="https://arxiv.org/abs/2008.03546">[Paper]</a></span>
					<span class="tag"><a href="./projects/OnlinePerson.html">[Webpage]</a></span>
					</td>
	</tbody>
	</table>

	
		
	<table style="margin-top: -1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
    <tbody><tr valign="baseline">
     	 <td width="160">
		<p align="center" style="margin-top:2mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/acl18hotflip.png" width="160"  border="0"></p>
        </td>
		<td  valign="middle">
		<p><span style="font-weight: bold;">HotFlip: White-Box Adversarial Examples for Text Classification</span> <br>
		Javid Ebrahimi, <span style="font-weight: bold;">Anyi Rao</span>, Daniel Lowd, Dejing Dou <br>
		Annual Meeting of the Association for Computational Linguistics
		(<span style="font-weight: bold;">ACL</span>), 2018 <br>
		It is included in several open-source NLP research libraries <a href="https://github.com/allenai/allennlp"> AllenNLP</a>,
		<a href="https://github.com/QData/TextAttack"> TextAttack</a>
		and <a href="https://github.com/thunlp/OpenAttack"> OpenAttack</a>
		<br>
		<span class="tag"><a href="https://aclanthology.org/P18-2006.pdf"> [Paper]</a></span>
		<span class="tag"><a href="http://www.aclweb.org/anthology/P18-2006"> [Webpage]</a></span>
		<span class="tag"><a href="https://github.com/allenai/allennlp/blob/master/allennlp/interpret/attackers/hotflip.py"> [AllenNLP]</a></span>
		<span class="tag"><a href="https://github.com/thunlp/OpenAttack/blob/master/OpenAttack/attackers/hotflip.py"> [OpenAttack]</a></span>
			 </td>
	</tbody>
	</table>
	
</div>


<div class="content anchor" id="experience">  	      
	<h3 class="content-section-list">Experience</h3>
	<p>
		<li>
			Research Intern at <a style="color:#333" href="https://about.meta.com/realitylabs">Meta Reality Lab</a>
		</li>	
		<li>
			Research Intern at <a style="color:#333" href="https://www.shlab.org.cn">Shanghai Artificial Intelligence Laboratory</a>
		</li>	
		<li>
			Research Intern at <a style="color:#333" href="https://www.sensetime.com/en">SenseTime Research</a>
		</li>
		<li>
			Visitor at the 
			<a style="color:#333" href="https://web.cs.toronto.edu">University of Toronto</a> and 
			<a style="color:#333" href="https://vectorinstitute.ai">Vector Institute</a>
		</li>
		<li>
			Research Assistant at the 
			<a style="color:#333" href="http://aimlab.cs.uoregon.edu/publication.php">Advanced Integration and Mining Lab</a>, 
			Eugene, OR, USA
		</li>
		<li>
			Research Intern at 
			<a style="color:#333" href="https://www.cs.hku.hk/">University of Hong Kong</a>,
			Hong Kong S.A.R.
		</li>
	</p>
</div>


<div class="content" >    
	<h3 class="content-section-list">Awards and Grant</h3>  
		<p>
			<table style="border-spacing:2px" width="100%">
			<tbody>
			<tr><td><li>
				<a href="https://www.primevideotech.com/"">
				Amazon</a> Gift Funding </td> <td align="right"> 2023 </td></tr>
			<tr><td><li>
				KAUST Grant for ECCV Workshop Organization</td> <td align="right"> 2022 </td></tr>
			<tr><td><li>
				Adobe Grant for ICCV Workshop Organization</td> <td align="right"> 2021 </td></tr>
			<tr><td><li>
				<a href="https://cerg1.ugc.edu.hk/hkpfs/index.html"> Hong Kong PhD Fellowship</a> </td> <td align="right"> 2021 </td></tr>
			<tr><td><li>
				<a style="color:#333" href="https://www.paperdigest.org/2021/08/most-influential-acl-papers-2021-08/"> Most Influential Paper by Paper Digest</a> </td> <td align="right"> 2021 </td></tr>
			<tr><td><li>
				National Scholarship awarded by the China Ministry of Education,
				the highest honor in China </td> <td align="right"> 2015 </td></tr>
			<tr><td><li>
				Provincial Merit Student awarded by the Jiangsu Province,
				the highest honor in the province </td> <td align="right"> 2017 </td></tr>
			<tr><td><li>
				<a style="color:#333" href="http://xgc.nju.edu.cn/08/f9/c1980a264441/page.htm"> Nanjing University Top-Grade Scholarship</a>,
				the highest honor in the university </td> <td align="right">2018</td></tr>
			<tr><td><li>
				SenseTime Scholarship, awarded to 30 students out of all AI major undergraduate students in China</td> <td align="right"> 2017 </td></tr>
			<tr><td><li> 
				<a style="color:#333" href="./images/more/MathSeniorHigh.png">Gold Medal</a> in Invitational National Mathematical Olympiad</td> <td align="right"> 2013</td></tr>
			</li>
			</tbody>
			</table>

			<details><summary>More</summary>
			<table style="border-spacing:2px" width="100%">
			<tbody>
			<tr><td><li>Nanjing University Outstanding Student Leader Award</td> <td align="right"> 2015</td></tr>
			<tr><td><li>Nanjing University Outstanding Student Award</td> <td align="right"> 2016</td></tr>
			<tr><td><li>Nanjing University Top Volunteer Excellence Award </td> <td align="right"> 2015</td></tr>
			<tr><td><li>Zhenggang Scholarship, top 40 students in Nanjing University </td> <td align="right"> 2016</td></tr>
			<tr><td><li>Zhenggang Jingying Scholarship </td> <td align="right"> 2017</td></tr>
			<tr><td><li>Nanjing University People Scholarship</td> <td align="right"> 2016</td></tr>
			<tr><td><li>Nanjing University People Scholarship</td> <td align="right"> 2017</td></tr>
			<tr><td><li>World ranking 32nd in 2016 <a style="color:#333" href="https://www.youtube.com/watch?v=iO5runHvnoM"> Calculus World Cup</a> </td> <td align="right"> 2016</td></tr>
			<tr><td><li>Meritorious winner prize in the 2016 National Mathematical Contest in Modeling  </td> <td align="right"> 2016</td></tr>
			<tr><td><li>Best paper in the 2014 University Electronics Design Contest  </td> <td align="right">2014</td></tr>
			</li>
			</tbody>
			</table>
			</details>
	</p>
</div>    

		
<div class="content anchor" id="undergrad">
	<h3 class="content-section">Early Stage Research and Academic</h3>
		<details><summary>Click to expand</summary>
		
		<h4>Early Stage Research</h4>
			<table style="margin-top:-1mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
			<tbody><tr valign="baseline">
				<td width="90" >
				<p align="center" style="margin-top:0mm; margin-right:2mm; margin-bottom:0; margin-left:0;" class=""><a href="http://mmlab.ie.cuhk.edu.hk/" target="_blank"><img src="./images/institute/mm.png" width="90" border="0"></a></p>
				</td>
				<td  valign="middle" >
				<p style="margin-top:0; margin-right:0; margin-bottom:2mm;"><span style="font-weight: bold;"> Multi-modal Video Analysis and Understanding</span> <br>
				<span style="font-style: italic;"> August. 2018 - August. 2020 </span> &nbsp  MMLab, Hong Kong S.A.R.<br>
				Advisor: Prof. <a href="http://dahua.me">Dahua Lin</a>  (Director) and Prof. <a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou</a> (Innovators Under 35) <br>
					<li>Cinematic style analysis in videos, published one ECCV20.<br>
					<li>Multi-modal story and plot understanding in movies, published one ECCV20.
					<li>Scene understanding in movies, published one ECCV20 and one CVPR20.
					</li>
					</p>
					</td>
			</tbody>
			</table>

			<table style="margin-top:-3mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
				<tbody><tr valign="baseline">
					<td width="90" >
					<p align="center" style="margin-top:3mm; margin-right:2mm; margin-bottom:0; margin-left:0;" class=""><a href="http://aimlab.cs.uoregon.edu" target="_blank"><img src="./images/institute/aim.jpg" width="90" border="0"></a></p>
					</td>
					<td valign="middle" >
					<p style="margin-top:0; margin-right:0; margin-bottom:2mm;"><span style="font-weight: bold;">Robust Training with Word-level Adversity for NLP</span> <br>
					<span style="font-style: italic;"> Sept. 2017 - April. 2018 </span> &nbsp  Advanced Integration and Mining Lab (AIM), Eugene, OR, United States of America<br>
					Advisor: Prof. <a href="http://ix.cs.uoregon.edu/~dou/">Dejing Dou</a>  (Director, Head of Baidu Big Data Lab) and Prof. <a href="http://ix.cs.uoregon.edu/~lowd/">Daniel Lowd</a> (Director of Graduate Studies) <br>
					<li>Proposed an efficient word-level gradient-based adversarial examples generation approach for training robust models.<br>
					<li>Evaluated the method across a wide range of sentence-level classification tasks 
						and the method using adversarial training achieved excellent performances on benchmarks.
					<li>This work on <a href="https://www.theregister.co.uk/2018/06/28/machine_translation_vulnerable/"> Adversarial examples for NLP</a> 
						was featured in an article in <a href="https://www.theregister.co.uk/">The Register</a>.
					</li>
					</p>
					</td>
			</tbody>
			</table>

		<h4>Undergrad Academic</h4>
			<p style="margin-top:-1mm;">His GPA ranked No.1 in each semester during his undergraduate studies at Nanjing University
			with an overall GPA: 3.96/4.00 and Rank: 1/183.
			He finished major curricula in 2 years and learned a bunch of online courses.
				<a href="./all_undergrad_online"> [Whole]</a><br>
			</p>

		<h4>Undergrad Research Beginning</h4>
			<table style="margin-top:-2mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
			<tbody><tr valign="baseline">
				<td width="90" >
				<p align="center" style="margin-top:4mm; margin-right:2mm; margin-bottom:0; margin-left:0;" class=""><a href="http://www.cs.hku.hk/" target="_blank"><img src="./images/institute/hku.jpg" width="90" border="0"></a></p>
				</td>
				<td  valign="middle" >
				<p style="margin-top:0; margin-right:0; margin-bottom:2mm;"><span style="font-weight: bold;">Automatic Music Accompaniment Using Probabilistic Machine Learning</span> <br>
				<span style="font-style: italic;"> Jul. 2017 - Aug. 2017 </span> &nbsp  The University of Hong Kong, Hong Kong S.A.R.<br>
				Advisor: Prof. <a href="http://www.cs.hku.hk/people/profile.jsp?teacher=fcmlau">Francis Lau</a> (Associate Dean) <br>
				<li>Proposed a fast decoding algorithm to deal with performance errors and reduce computational complexity from O(n<sup>2</sup>)
				to O(n). It is able to work in real-time with practical length scores.<br>
				<li>Constructed a comprehensive system and developed a parallel Hidden Markov Model for score following.<br>
				<li>Developed a new free open-source Windows-based automatic music follower and accompanist.		
				</li>
				</p>
					 </td>
			</tbody>
			</table>

			<table style="margin-top:-3mm;" bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
			<tbody><tr valign="baseline">
				<td width="90" >
				<p align="center" style="margin-top:4mm; margin-right:2mm; margin-bottom:0; margin-left:0;" class=""><a href="https://www.nju.edu.cn/EN/" target="_blank"><img src="./images/institute/nju.jpg" width="90" border="0"></a></p>
				</td>
				<td  valign="middle" >
				<p style="margin-top:0; margin-right:0; margin-bottom:2mm;"><span style="font-weight: bold;">Real-time 3D Surface Reconstruction Using Lidar (Light Detection And Ranging)</span> <br>
				<span style="font-style: italic;"> Aug. 2016 - Sept. 2017 </span> &nbsp  Visual Sensing and Graphics Lab (VISG Lab), Nanjing University <br>
				Supervisor: Prof. <a href="https://ese.nju.edu.cn/dsd/list.htm">  Sidan Du</a> (Director)<br>
				<li>Proposed a novel line of sight algorithm to real-time reconstruct surface and achieved state-of-the-art results.<br>
				<li>Employed a new surface lattice data structure in implicit surface update for memory efficiency.<br>
				<li>Presented a real-time 3D reconstruction pipeline for large-scale Lidar point clouds.<br>
				<li>Implemented parallel computation to update the implicit surface faster and Motion Estimation and Mapping to register point clouds.
				<span class="tag"><a href="papers/icra2018pre.pdf"> [Report]</a></span> 
				<span class="tag"><a href="https://youtu.be/_6JFl3p75qg"> [Video]</a></span>
				</li>	
				</p>
				</td>
			</tbody>
			</table>	

		<h4>Undergrad Course Projects</h4>
			<li>Computer Vision:
				3D Human Poses Estimation from a Single Image
				<span class="tag"><a href="papers/CV_Report.pdf"> [Presentation]</a></span>
				Reduced the ambiguities in the 3D pose estimation using sparse coding and  
				applied human-portion constraints to acquire a minimization problem.
			</li>

			<li>Convex Optimization:
				Road Car Flow Prediction
				<span class="tag"><a href="papers/mcm.pdf"> [Report]</a></span>
				Predicted car motion under road circumstances constraints using 
				Markov Decision Process and OD estimated matrix,
				and improved the performance according to different traffic loads and vehicle types.
			</li>

			<li>Probability and Stochastic Process:
				Monte Carlo for Multidimensional Integrals
				<span class="tag"><a href="papers/monte_carlo_method.pdf"> [Report]</a></span>	
			</li>

			<li>Machine Learning:
				Spam Filtering System Construction
			</li>

			<li>Microcomputers and Interface Techniques:
				x86 Assembly Language Programming
				<span class="tag"><a href="papers/masm.pdf"> [Report]</a></span>
			</li>

			<li>Signal Processing: Single-Photon Detector Design
				<span class="tag"><a href="papers/basic_forum.pdf"> [Report]</a></span>
				<span class="tag"><a href="papers/Low_noise_preamplifier.pdf"> [Presentation]</a></span>
				Designed a 64-channel low-noise pre-amplifier using a symmetric structure with 100 times less noise, 
				and drew its 8-layers circuit board.
			</li>
		</details>
	<br>
</div>

	
<div class="content" >      
	<h3 class="content-section-expand">Volunteer and Leadership Experience</h3>
		<details><summary>Click to expand</summary>
		<p> <span style="font-weight: bold;"> Co-Founder </span> of a Children Care Volunteer Program  &nbsp &nbsp 
			<span style="font-style: italic;"> Sep. 2015 - Dec. 2015</span> <br>		
			Co-founded a psychological consulting program to promote left-behind children's growth and education.
			Volunteered to teach left-behind children Math and English in 
			a junior high school located in the remote, underdeveloped Xiushui county.
			Recognized as a key team leader in the successful Warm One Hundred Campaign, which raised money for
			left-behind children. Our group received an excellence award from the China Foundation for Poverty Alleviation.
		</p>

		<p> <span style="font-weight: bold;"> Vice President </span> of a Young Volunteers Association at Nanjing University &nbsp &nbsp
			<span style="font-style: italic;"> Jun. 2015 - Jun. 2016</span> <br>
			Organized and participated in over 100 out-of-school and 20 in-school activities covering over 1000 volunteers.
			Our association received a volunteer association excellence award.
		</p>

		<p> <span style="font-weight: bold;"> Research Intern Group Leader </span> of <a style="color:#333" href=" http://www.cj-elec.com/en/">JCET</a> 
			<span class="tag"><a href="https://ese.nju.edu.cn/87/35/c22536a362293/page.htm"> [Media Report]</a></span> &nbsp &nbsp
			<span style="font-style: italic;"> July. 2015 - Aug. 2015</span><br>
		</p>
		
		<p> <span style="font-weight: bold;"> Campus Ambassador </span> of <a style="color:#333" href="http://www.huawei.com/en/">Huawei</a> &nbsp &nbsp
			<span style="font-style: italic;"> Aug. 2017 - Dec. 2017</span> <br>
		</p>

		<p> <span style="font-weight: bold;"> Student Volunteer </span> of International Conference on Computer Vision (ICCV) &nbsp &nbsp 
			<span style="font-style: italic;"> Dec. 2019</span> <br>
		</p>
		</details>
	<br>
</div> 


<div class="content" >
	<h3 class="content-section-mis">Miscellaneous</h3>
		<p> <span style="font-weight: bold;"> Professional Activities </span> <br>
				Co-organizer: 
				<a style="color:#333" href="https://cveu.github.io/">Workshop on AI for Creative Video Editing and Understanding</a> at ECCV 2022, ICCV 2021 
				<br>
				Program Committee Member: CVPR, ICCV, CHI, NeurIPS, ICML, ICLR, AAAI, IJCAI, ECCV<br>
				Journal Reviewer: IEEE Transactions on Multimedia, IEEE Transactions on Circuits and Systems for Video Technology<br>
				Judge: 
				<a style="color:#333" href="https://www.youtube.com/watch?v=IrBnULdzx1E&ab_channel=SenseTime%E5%95%86%E6%B9%AF%E7%A7%91%E6%8A%80">
				The 3rd International Artificial Intelligence Fair</a>
		</p>

		<p class="content-subsection-mis"> <span style="font-weight: bold;"> Teaching Experience </span> <br>
			Head TA, IERG 4160, Image and Video Processing (graduate level), <span style="font-style: italic;">Fall 2019</span> <br>
			TA, IERG 3180, Microcontrollers and Embedded Systems Laboratory, <span style="font-style: italic;">Spring 2020</span> 
		</p>

		<p class="content-subsection-mis"> <span style="font-weight: bold;">Patents</span><br>
			A Video Generation Method, CN202210699177.X <br>
			A Video Editing Method and Related Program Products, CN202210691662.2 <br>
			A Video Editing Method, CN202010694551.1 <br>
			A Video Classification Method, CN202010694811.1 <br>
			An Image Processing Method and Related Products, CN202010450801.3  <br>
			A Zero-shot Action Recognition Method, CN202110821209.4 <br>
			A Layout Generation Method, CN202111128490.X
		</p>

		<p class="content-subsection-mis"> <span style="font-weight: bold;">Hobbies</span><br>
		Love: 🌊🥥🏄‍♂️✈️🎬 <br>
		Travel: 🇨🇳🇭🇰🇺🇸🇯🇵🇲🇴🇬🇧🇸🇬🇰🇷🇦🇪🇳🇱🇧🇪🇩🇪🇫🇷🇹🇭<br>
		Undergrad:
		University <a style="color:#333" href="http://www.nju.edu.cn/EN/84/08/c7125a164872/page.htm"> Student Choir</a> Member
			(<span class="tag"><a style="color:#333" href="http://v.youku.com/v_show/id_XMjA1ODIzMjQwOA==.html?spm=a2hzp.8244740.userfeed.5!2~5~5~5!3~5~A.Rb6hYG">Joyful Snowflakes </a></span>  written by Chih-mo Hsu),
		Department Young Volunteers Association Vice President,
		Bronze medal at University 55th Sports Meeting <br>
		Language: A bit of Japanese and Cantonese, Native Mandarin, Full proficiency in English
		</p>
	</div>

	<div style="text-align:center;">
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=2d78ad&w=250&t=tt&d=NoxTntTwijcGNPPsm7CM_ctT1E79_SAsBke-aS4Vw4Q&co=ffffff&ct=2d78ad&cmo=aa3939&cmn=ff5353'></script>
	</div>
</body>
